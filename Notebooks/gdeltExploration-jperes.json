{"paragraphs":[{"text":"%md\n## Exploration des donnees GDELT via Spark\nDans ce notebook nous allons commencer a explorer les donnees GDELT qu'on a stoque sur S3","user":"anonymous","dateUpdated":"2020-01-22T15:47:47+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Exploration des donnees GDELT via Spark</h2>\n<p>Dans ce notebook nous allons commencer a explorer les donnees GDELT qu&rsquo;on a stoque sur S3</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1579708067886_1255639685","id":"20181212-102323_67420128","dateCreated":"2020-01-22T15:47:47+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2023"},{"text":"import org.apache.spark.sql.functions._\n","user":"anonymous","dateUpdated":"2020-01-22T15:47:52+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions._\n"}]},"apps":[],"jobName":"paragraph_1579708067889_-1348378660","id":"20200117-113234_348261115","dateCreated":"2020-01-22T15:47:47+0000","dateStarted":"2020-01-22T15:47:53+0000","dateFinished":"2020-01-22T15:48:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2024"},{"text":"import com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicAWSCredentials\nimport com.amazonaws.auth.BasicSessionCredentials\nval AWS_ID = \"ASIA4XEAYHWQQ2TY63FV\"\nval AWS_KEY = \"MlBkyotD00NqQvsoEi+mX6776SVx4ycrSV1OyRSL\"\nval AWS_TOKEN = \"FwoGZXIvYXdzEKj//////////wEaDKMy85A+wFoGN8auBSK9ASAt3bH5AvSBcnB+S7i7hHhA0eF4iRo/YCNPxteRLodWRdTmsZZei/2ps0gvyRXqWAcq5aJwfnBLxsuFSXlzfCYxAou2MonLwSe29dklWlHDmB0LKSBlcSqnhrvC5fkGjfBGPWsjUbFnnyLw0ZvIoCHH0y3WyWLwwhvDPNakzC8zMSoTSBnz9B7SrZgPEmWd/am4Nqz8jxN2dkKGYcMojyA2FGbfPBzK1HF6yFW/rQeA9HJBubTDovDMrrg2VCjMtKHxBTItvHu0jDE2Nb4cfH1FrBr7u8YTaX1ADDCtGzBV1fZ4gg4EBT4Cum67OM8Wznmc\"\n// la classe AmazonS3Client n'est pas serializable\n// on rajoute l'annotation @transient pour dire a Spark de ne pas essayer de serialiser cette classe et l'envoyer aux executeurs\n@transient val awsClient = new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY, AWS_TOKEN) )\nsc.hadoopConfiguration.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\nsc.hadoopConfiguration.set(\"fs.s3a.access.key\", AWS_ID) // mettre votre ID du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.secret.key\", AWS_KEY) // mettre votre secret du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.session.token\", AWS_TOKEN)","user":"anonymous","dateUpdated":"2020-01-22T15:49:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one deprecation warning; re-run with -deprecation for details\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicAWSCredentials\nimport com.amazonaws.auth.BasicSessionCredentials\nAWS_ID: String = ASIA4XEAYHWQQ2TY63FV\nAWS_KEY: String = MlBkyotD00NqQvsoEi+mX6776SVx4ycrSV1OyRSL\nAWS_TOKEN: String = FwoGZXIvYXdzEKj//////////wEaDKMy85A+wFoGN8auBSK9ASAt3bH5AvSBcnB+S7i7hHhA0eF4iRo/YCNPxteRLodWRdTmsZZei/2ps0gvyRXqWAcq5aJwfnBLxsuFSXlzfCYxAou2MonLwSe29dklWlHDmB0LKSBlcSqnhrvC5fkGjfBGPWsjUbFnnyLw0ZvIoCHH0y3WyWLwwhvDPNakzC8zMSoTSBnz9B7SrZgPEmWd/am4Nqz8jxN2dkKGYcMojyA2FGbfPBzK1HF6yFW/rQeA9HJBubTDovDMrrg2VCjMtKHxBTItvHu0jDE2Nb4cfH1FrBr7u8YTaX1ADDCtGzBV1fZ4gg4EBT4Cum67OM8Wznmc\nawsClient: com.amazonaws.services.s3.AmazonS3Client = com.amazonaws.services.s3.AmazonS3Client@484c4709\n"}]},"apps":[],"jobName":"paragraph_1579708067890_1827267724","id":"20200117-095750_116093939","dateCreated":"2020-01-22T15:47:47+0000","dateStarted":"2020-01-22T15:49:18+0000","dateFinished":"2020-01-22T15:49:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2025"},{"text":"%md Les fichiers sont stoquees compresses, on a besoin d'un bout de code pour les decompresser en parallel sur les workers au fur et a mesure qu'on les lit depuis S3:","user":"anonymous","dateUpdated":"2020-01-22T15:47:47+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Les fichiers sont stoquees compresses, on a besoin d&rsquo;un bout de code pour les decompresser en parallel sur les workers au fur et a mesure qu&rsquo;on les lit depuis S3:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1579708067890_1598038145","id":"20181212-102329_808049084","dateCreated":"2020-01-22T15:47:47+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2026"},{"text":"import org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\n// 20181201000000.export.CSV.zip\n\nval gkgRDD = sc.binaryFiles(\"s3a://jperes-gdelt/201812[0-9]*.gkg.csv.zip\"). // charger quelques fichers via une regex\n   flatMap {  // decompresser les fichiers\n       case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n                takeWhile(_ != null).\n                flatMap { _ =>\n                    val br = new BufferedReader(new InputStreamReader(zis))\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\n                }\n    }","user":"anonymous","dateUpdated":"2020-01-22T15:49:58+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\ngkgRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at flatMap at <console>:38\n"}]},"apps":[],"jobName":"paragraph_1579708067890_-1914980472","id":"20171217-232457_1732696781","dateCreated":"2020-01-22T15:47:47+0000","dateStarted":"2020-01-22T15:49:58+0000","dateFinished":"2020-01-22T15:50:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2027"},{"text":"\nval gkgDF_q3 = gkgRDD.map(_.split(\"\\t\")).map(e=> (e(0), e(1), e(3), e(7), e(9), e(11), e(15))).toDF(\"GKGRECORDID\", \"DATE\", \"SourceCommonName\",\"Themes\", \"Locations\", \"Persons\", \"Tone\")","user":"anonymous","dateUpdated":"2020-01-22T15:50:01+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"gkgDF_q3: org.apache.spark.sql.DataFrame = [GKGRECORDID: string, DATE: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1579708067891_498154908","id":"20200117-092909_1363473631","dateCreated":"2020-01-22T15:47:47+0000","dateStarted":"2020-01-22T15:50:01+0000","dateFinished":"2020-01-22T15:50:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2028"},{"text":"gkgDF_q3.show(5)","user":"anonymous","dateUpdated":"2020-01-22T15:47:47+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579708067891_-912512137","id":"20200118-174830_69069544","dateCreated":"2020-01-22T15:47:47+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2029"},{"text":"def clean_location(raw: String) : Array[String] = {\n    val arr = raw.replaceAll(\"\\\\s\", \"\").split(\"\\\\,|#\")\n    var city = \"\"\n    var state = \"\"\n    var country = \"\"\n    if (arr(0) == \"1\") {\n        country = arr(1)\n    }\n    else if (arr(0) == \"2\") {\n        state = arr(1)\n        country = arr(2)\n    }\n    else if (arr(0) == \"3\") {\n        city = arr(1)\n        state = arr(2)\n        country = arr(3)\n    }\n    else if (arr(0) == \"4\") {\n        city = arr(1)\n        state = arr(2)\n        country = arr(3)\n    }\n    else if (arr(0) == \"5\") {\n        state = arr(1)\n        country = arr(2)\n    }\n    return Array(city, state, country)\n}\n\nval udf_clean_location = udf(clean_location _)","user":"anonymous","dateUpdated":"2020-01-22T15:49:38+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"clean_location: (raw: String)Array[String]\nudf_clean_location: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,ArrayType(StringType,true),Some(List(StringType)))\n"}]},"apps":[],"jobName":"paragraph_1579708067891_-889918746","id":"20200117-145512_888055607","dateCreated":"2020-01-22T15:47:47+0000","dateStarted":"2020-01-22T15:49:38+0000","dateFinished":"2020-01-22T15:49:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2030"},{"text":"def avg_tone(raw: String) : Double = {raw.split(\",\")(0).toDouble}\n\nval udf_avg_tone = udf(avg_tone _)","user":"anonymous","dateUpdated":"2020-01-22T15:49:40+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"avg_tone: (raw: String)Double\nudf_avg_tone: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,DoubleType,Some(List(StringType)))\n"}]},"apps":[],"jobName":"paragraph_1579708067892_-1404960789","id":"20200117-153051_869221973","dateCreated":"2020-01-22T15:47:47+0000","dateStarted":"2020-01-22T15:49:40+0000","dateFinished":"2020-01-22T15:49:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2031"},{"text":"def raw_to_date(raw: String) : Double = {to_date(raw.take(8),\"yyyyMMdd\")}\n\nval udf_avg_tone = udf(raw_to_date _)\n\nraw_to_date(\"19941201dff343\")","user":"anonymous","dateUpdated":"2020-01-22T15:49:42+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"<console>:29: error: overloaded method value to_date with alternatives:\n  (e: org.apache.spark.sql.Column,fmt: String)org.apache.spark.sql.Column <and>\n  (e: org.apache.spark.sql.Column)org.apache.spark.sql.Column\n cannot be applied to (String, String)\n       def raw_to_date(raw: String) : Double = {to_date(raw.take(8),\"yyyyMMdd\")}\n                                                ^\n"}]},"apps":[],"jobName":"paragraph_1579708067892_1036698165","id":"20200117-160605_544594025","dateCreated":"2020-01-22T15:47:47+0000","dateStarted":"2020-01-22T15:49:42+0000","dateFinished":"2020-01-22T15:49:43+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:2032"},{"text":"val gkgDF_q3_cleaned = gkgDF_q3\n    .withColumn(\"City\", udf_clean_location($\"Locations\")(0))\n    .withColumn(\"State\", udf_clean_location($\"Locations\")(1))\n    .withColumn(\"Country\", udf_clean_location($\"Locations\")(2))\n    .withColumn(\"SourceCommonName\",trim($\"SourceCommonName\"))\n    .withColumn(\"Tone\", udf_avg_tone($\"Tone\"))\n    .withColumn(\"Themes\", split($\"Themes\", \"\\\\;\"))\n    .withColumn(\"Themes\",explode($\"Themes\"))\n    .withColumn(\"Themes\",trim($\"Themes\"))\n    .withColumn(\"Locations\", split($\"Locations\", \"\\\\;\"))\n    .withColumn(\"Locations\",explode($\"Locations\"))\n    .withColumn(\"Locations\",trim($\"Locations\"))\n    .withColumn(\"Persons\", split($\"Persons\", \"\\\\;\"))\n    .withColumn(\"Persons\",explode($\"Persons\"))\n    .withColumn(\"Persons\",trim($\"Persons\"))\n    .withColumn(\"Year\", substring($\"DATE\",0,4))\n    .withColumn(\"Month\", substring($\"DATE\",5,2))\n    .withColumn(\"Day\", substring($\"DATE\",7,2))\n    .drop(\"DATE\",\"Locations\")","user":"anonymous","dateUpdated":"2020-01-22T15:53:13+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"gkgDF_q3_cleaned: org.apache.spark.sql.DataFrame = [GKGRECORDID: string, SourceCommonName: string ... 9 more fields]\n"}]},"apps":[],"jobName":"paragraph_1579708067892_1485230398","id":"20200117-145150_1235311770","dateCreated":"2020-01-22T15:47:47+0000","dateStarted":"2020-01-22T15:53:13+0000","dateFinished":"2020-01-22T15:53:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2033"},{"text":"gkgDF_q3_cleaned.show(5)","user":"anonymous","dateUpdated":"2020-01-22T15:53:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------------+----------------+--------------------+-------+------+---------+-------------+------------+----+-----+---+\n|     GKGRECORDID|SourceCommonName|              Themes|Persons|  Tone|     City|        State|     Country|Year|Month|Day|\n+----------------+----------------+--------------------+-------+------+---------+-------------+------------+----+-----+---+\n|20181201000000-0|       nbc29.com|UNGP_FORESTS_RIVE...|       |-3.125|Fitchburg|Massachusetts|UnitedStates|2018|   12| 01|\n|20181201000000-0|       nbc29.com|UNGP_FORESTS_RIVE...|       |-3.125|Fitchburg|Massachusetts|UnitedStates|2018|   12| 01|\n|20181201000000-0|       nbc29.com|  TAX_WORLDLANGUAGES|       |-3.125|Fitchburg|Massachusetts|UnitedStates|2018|   12| 01|\n|20181201000000-0|       nbc29.com|  TAX_WORLDLANGUAGES|       |-3.125|Fitchburg|Massachusetts|UnitedStates|2018|   12| 01|\n|20181201000000-0|       nbc29.com|TAX_WORLDLANGUAGE...|       |-3.125|Fitchburg|Massachusetts|UnitedStates|2018|   12| 01|\n+----------------+----------------+--------------------+-------+------+---------+-------------+------------+----+-----+---+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1579708067892_138521040","id":"20200117-112628_323012166","dateCreated":"2020-01-22T15:47:47+0000","dateStarted":"2020-01-22T15:53:16+0000","dateFinished":"2020-01-22T15:53:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2034"},{"text":"gkgDF_q3_cleaned.createOrReplaceTempView(\"Test\") ","user":"anonymous","dateUpdated":"2020-01-22T16:20:58+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1579710044984_-1896059862","id":"20200122-162044_1472208117","dateCreated":"2020-01-22T16:20:44+0000","dateStarted":"2020-01-22T16:20:58+0000","dateFinished":"2020-01-22T16:20:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2035"},{"text":"spark.sql(\"\"\"SELECT distinct(Themes) FROM Test WHERE SourceCommonName = 'nbc29.com'\"\"\").show()","user":"anonymous","dateUpdated":"2020-01-22T16:23:12+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 4.0 failed 4 times, most recent failure: Lost task 2.3 in stage 4.0 (TID 34, ip-172-31-11-182.ec2.internal, executor 6): java.io.InterruptedIOException: getFileStatus on s3a://jperes-gdelt/20181201091500.gkg.csv.zip: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:125)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1571)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:521)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:790)\n\tat org.apache.spark.input.PortableDataStream.open(PortableDataStream.scala:183)\n\tat $anonfun$1.apply(<console>:40)\n\tat $anonfun$1.apply(<console>:38)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1175)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1121)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4926)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4872)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1321)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1295)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\n\t... 43 more\nCaused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool\n\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager.leaseConnection(PoolingHttpClientConnectionManager.java:314)\n\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager$1.get(PoolingHttpClientConnectionManager.java:280)\n\tat sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70)\n\tat com.amazonaws.http.conn.$Proxy32.get(Unknown Source)\n\tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:190)\n\tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\n\tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1297)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n\t... 56 more\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:401)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:751)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:710)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:719)\n  ... 49 elided\nCaused by: java.io.InterruptedIOException: getFileStatus on s3a://jperes-gdelt/20181201091500.gkg.csv.zip: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n  at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:125)\n  at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1571)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:521)\n  at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:790)\n  at org.apache.spark.input.PortableDataStream.open(PortableDataStream.scala:183)\n  at $anonfun$1.apply(<console>:40)\n  at $anonfun$1.apply(<console>:38)\n  at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n  at scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n  at scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n  at scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.agg_doAggregateWithKeys_0$(Unknown Source)\n  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1175)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1121)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n  at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n  at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4926)\n  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4872)\n  at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1321)\n  at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1295)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\n  ... 43 more\nCaused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool\n  at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.leaseConnection(PoolingHttpClientConnectionManager.java:314)\n  at org.apache.http.impl.conn.PoolingHttpClientConnectionManager$1.get(PoolingHttpClientConnectionManager.java:280)\n  at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70)\n  at com.amazonaws.http.conn.$Proxy32.get(Unknown Source)\n  at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:190)\n  at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\n  at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\n  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\n  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\n  at com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1297)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n  ... 56 more\n"}]},"apps":[],"jobName":"paragraph_1579708067893_1253968312","id":"20200118-172519_2047932866","dateCreated":"2020-01-22T15:47:47+0000","dateStarted":"2020-01-22T16:23:12+0000","dateFinished":"2020-01-22T16:26:02+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:2036"},{"user":"anonymous","dateUpdated":"2020-01-22T15:47:47+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579708067894_-1565239602","id":"20200118-172655_2069013167","dateCreated":"2020-01-22T15:47:47+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2037"}],"name":"gdeltExploration","id":"2EZX3USJY","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}