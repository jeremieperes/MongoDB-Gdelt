{"paragraphs":[{"text":"%md\n## Préparation des donnees GDELT via Spark\nDans ce notebook nous allons préparer les données GDELT via Spark puis les transférer dans des collections MongoDB","user":"anonymous","dateUpdated":"2020-01-22T22:55:25+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Préparation des donnees GDELT via Spark</h2>\n<p>Dans ce notebook nous allons préparer les données GDELT via Spark puis les transférer dans des collections MongoDB</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1579714639564_-441725231","id":"20181212-102323_67420128","dateCreated":"2020-01-22T17:37:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3017","dateFinished":"2020-01-22T22:55:25+0000","dateStarted":"2020-01-22T22:55:25+0000"},{"text":"%md\n# Load GDELT Databases","user":"anonymous","dateUpdated":"2020-01-22T22:56:01+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579733727038_1451149357","id":"20200122-225527_352819947","dateCreated":"2020-01-22T22:55:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5210","dateFinished":"2020-01-22T22:56:01+0000","dateStarted":"2020-01-22T22:56:01+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Load GDELT Databases</h1>\n</div>"}]}},{"text":"import org.apache.spark.sql.functions._\n\nimport com.mongodb.spark._\nimport com.mongodb.spark.config._\n\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicAWSCredentials\n\nimport org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\n    \nval AWS_ID = \"AKIAS4U3P2UU5U4JOWXX\"\nval AWS_KEY = \"KIsg3ZwFTBsw36IgOK/+UqNtOK8pTVS/cMrId/yH\"\n// la classe AmazonS3Client n'est pas serializable\n// on rajoute l'annotation @transient pour dire a Spark de ne pas essayer de serialiser cette classe et l'envoyer aux executeurs\n@transient val awsClient = new AmazonS3Client(new BasicAWSCredentials(AWS_ID, AWS_KEY))\n\nsc.hadoopConfiguration.set(\"fs.s3a.access.key\", AWS_ID) // mettre votre ID du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.secret.key\", AWS_KEY) // mettre votre secret du fichier credentials.csv","user":"anonymous","dateUpdated":"2020-01-22T22:57:56+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.functions._\nimport com.mongodb.spark._\nimport com.mongodb.spark.config._\n"}]},"apps":[],"jobName":"paragraph_1579714639565_334064980","id":"20200117-113234_348261115","dateCreated":"2020-01-22T17:37:19+0000","dateStarted":"2020-01-22T21:44:05+0000","dateFinished":"2020-01-22T21:44:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3018"},{"text":"val gkgRDD = sc.binaryFiles(\"s3://fpsx-telecom-gdelt2019/201912[0-9]*.gkg.csv.zip\").\n    flatMap { // decompresser les fichiers\n        case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n            takeWhile{ case null => zis.close(); false\n            case _ => true }.\n            flatMap { _ =>\n              val br = new BufferedReader(new InputStreamReader(zis))\n              Stream.continually(br.readLine()).takeWhile(_ != null)\n            }\n      }\n\nval exportRDD = sc.binaryFiles(\"s3://fpsx-telecom-gdelt2019/201912[0-9]*.export.CSV.zip\").\n    flatMap { // decompresser les fichiers\n        case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n            takeWhile{ case null => zis.close(); false\n            case _ => true }.\n            flatMap { _ =>\n              val br = new BufferedReader(new InputStreamReader(zis))\n              Stream.continually(br.readLine()).takeWhile(_ != null)\n            }\n      }\n\nval mentionsRDD = sc.binaryFiles(\"s3://fpsx-telecom-gdelt2019/201912[0-9]*.export.CSV.zip\").\n    flatMap { // decompresser les fichiers\n        case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n            takeWhile{ case null => zis.close(); false\n            case _ => true }.\n            flatMap { _ =>\n              val br = new BufferedReader(new InputStreamReader(zis))\n              Stream.continually(br.readLine()).takeWhile(_ != null)\n            }\n      }","user":"anonymous","dateUpdated":"2020-01-22T22:57:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\ngkgRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[405] at flatMap at <console>:243\nexportRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[407] at flatMap at <console>:256\nmentionsRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[409] at flatMap at <console>:269\n"}]},"apps":[],"jobName":"paragraph_1579714639566_-1825853887","id":"20171217-232457_1732696781","dateCreated":"2020-01-22T17:37:19+0000","dateStarted":"2020-01-22T22:57:04+0000","dateFinished":"2020-01-22T22:57:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3021"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579733555338_-141379231","id":"20200122-225235_1981745834","dateCreated":"2020-01-22T22:52:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4979","text":"%md\n# Query 3","dateUpdated":"2020-01-22T22:52:44+0000","dateFinished":"2020-01-22T22:52:44+0000","dateStarted":"2020-01-22T22:52:44+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Query 3</h1>\n</div>"}]}},{"text":"val gkgDF_q3 = gkgRDD.map(_.split(\"\\t\")).map(e=> (e(0), e(1), e(3), e(7), e(9), e(11), e(15))).toDF(\"GKGRECORDID\", \"DATE\", \"SourceCommonName\",\"Themes\", \"Locations\", \"Persons\", \"Tone\")","user":"anonymous","dateUpdated":"2020-01-22T21:52:55+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"gkgDF_q3: org.apache.spark.sql.DataFrame = [GKGRECORDID: string, DATE: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1579714639566_-125805869","id":"20200117-092909_1363473631","dateCreated":"2020-01-22T17:37:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3022","dateFinished":"2020-01-22T21:52:56+0000","dateStarted":"2020-01-22T21:52:55+0000"},{"text":"def clean_location(raw: String) : Array[String] = {\n    val arr = raw.replaceAll(\"\\\\s\", \"\").split(\"\\\\,|#\")\n    var city = \"\"\n    var state = \"\"\n    var country = \"\"\n    if (arr(0) == \"1\") {\n        country = arr(1)\n    }\n    else if (arr(0) == \"2\") {\n        state = arr(1)\n        country = arr(2)\n    }\n    else if (arr(0) == \"3\") {\n        city = arr(1)\n        state = arr(2)\n        country = arr(3)\n    }\n    else if (arr(0) == \"4\") {\n        city = arr(1)\n        state = arr(2)\n        country = arr(3)\n    }\n    else if (arr(0) == \"5\") {\n        state = arr(1)\n        country = arr(2)\n    }\n    return Array(city, state, country)\n}\n\ndef avg_tone(raw: String) : Double = {raw.split(\",\")(0).toDouble}\n\ndef raw_to_date(raw: String) : Double = {to_date(raw.take(8),\"yyyyMMdd\")}\n\n\n\nval udf_avg_tone = udf(raw_to_date _)\n\nval udf_avg_tone = udf(avg_tone _)\n\nval udf_clean_location = udf(clean_location _)","user":"anonymous","dateUpdated":"2020-01-22T22:53:32+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"clean_location: (raw: String)Array[String]\nudf_clean_location: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,ArrayType(StringType,true),Some(List(StringType)))\n"}]},"apps":[],"jobName":"paragraph_1579714639567_1022226237","id":"20200117-145512_888055607","dateCreated":"2020-01-22T17:37:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3024","dateFinished":"2020-01-22T21:53:01+0000","dateStarted":"2020-01-22T21:53:01+0000"},{"text":"val gkgDF_q3_cleaned = gkgDF_q3\n    .withColumn(\"City\", udf_clean_location($\"Locations\")(0))\n    .withColumn(\"State\", udf_clean_location($\"Locations\")(1))\n    .withColumn(\"Country\", udf_clean_location($\"Locations\")(2))\n    .withColumn(\"SourceCommonName\",trim($\"SourceCommonName\"))\n    .withColumn(\"Tone\", udf_avg_tone($\"Tone\"))\n    .withColumn(\"Themes\", split($\"Themes\", \"\\\\;\"))\n    .withColumn(\"Themes\",explode($\"Themes\"))\n    .withColumn(\"Themes\",trim($\"Themes\"))\n    .withColumn(\"Locations\", split($\"Locations\", \"\\\\;\"))\n    .withColumn(\"Locations\",explode($\"Locations\"))\n    .withColumn(\"Locations\",trim($\"Locations\"))\n    .withColumn(\"Persons\", split($\"Persons\", \"\\\\;\"))\n    .withColumn(\"Persons\",explode($\"Persons\"))\n    .withColumn(\"Persons\",trim($\"Persons\"))\n    .withColumn(\"Year\", substring($\"DATE\",0,4))\n    .withColumn(\"Month\", substring($\"DATE\",5,2))\n    .withColumn(\"Day\", substring($\"DATE\",7,2))\n    .drop(\"DATE\",\"Locations\")","user":"anonymous","dateUpdated":"2020-01-22T21:53:05+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"gkgDF_q3_cleaned: org.apache.spark.sql.DataFrame = [GKGRECORDID: string, SourceCommonName: string ... 9 more fields]\n"}]},"apps":[],"jobName":"paragraph_1579714639568_-1479838568","id":"20200117-145150_1235311770","dateCreated":"2020-01-22T17:37:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3027","dateFinished":"2020-01-22T21:53:06+0000","dateStarted":"2020-01-22T21:53:05+0000"},{"text":"gkgDF_q3_cleaned.show(5)","user":"anonymous","dateUpdated":"2020-01-22T21:53:09+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------------+----------------+--------------------+-------+-----------------+----+-----+-----------+----+-----+---+\n|     GKGRECORDID|SourceCommonName|              Themes|Persons|             Tone|City|State|    Country|Year|Month|Day|\n+----------------+----------------+--------------------+-------+-----------------+----+-----+-----------+----+-----+---+\n|20191201000000-0|     carbuzz.com|  TAX_WORLDLANGUAGES|       |0.792393026941363|    |     |SaudiArabia|2019|   12| 01|\n|20191201000000-0|     carbuzz.com|  TAX_WORLDLANGUAGES|       |0.792393026941363|    |     |SaudiArabia|2019|   12| 01|\n|20191201000000-0|     carbuzz.com|TAX_WORLDLANGUAGE...|       |0.792393026941363|    |     |SaudiArabia|2019|   12| 01|\n|20191201000000-0|     carbuzz.com|TAX_WORLDLANGUAGE...|       |0.792393026941363|    |     |SaudiArabia|2019|   12| 01|\n|20191201000000-0|     carbuzz.com|           MEDIA_MSM|       |0.792393026941363|    |     |SaudiArabia|2019|   12| 01|\n+----------------+----------------+--------------------+-------+-----------------+----+-----+-----------+----+-----+---+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1579714639568_1991151436","id":"20200117-112628_323012166","dateCreated":"2020-01-22T17:37:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3028","dateFinished":"2020-01-22T21:53:09+0000","dateStarted":"2020-01-22T21:53:09+0000"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579733654127_-288775203","id":"20200122-225414_1760322409","dateCreated":"2020-01-22T22:54:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5093","text":"%md \n\n# Write to MongoDB","dateUpdated":"2020-01-22T22:54:24+0000","dateFinished":"2020-01-22T22:54:24+0000","dateStarted":"2020-01-22T22:54:24+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Write to MongoDB</h1>\n</div>"}]}},{"text":"def writeIntoMongodb(df: org.apache.spark.sql.DataFrame, collection_name: String) = {\n    MongoSpark.save(df.write.option(\"spark.mongodb.output.uri\", \"mongodb://gdeltuser:gdeltpass@ec2-15-188-82-40.eu-west-3.compute.amazonaws.com:27017,ec2-15-188-185-108.eu-west-3.compute.amazonaws.com:27017,ec2-35-180-90-165.eu-west-3.compute.amazonaws.com:27017/gdelt.\"+collection_name+\"?replicaSet=rsGdelt\").mode(\"overwrite\"))\n}","user":"anonymous","dateUpdated":"2020-01-22T22:19:59+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"writeIntoMongodb: (df: org.apache.spark.sql.DataFrame, collection_name: String)Unit\n"}]},"apps":[],"jobName":"paragraph_1579714639569_761929092","id":"20200118-172519_2047932866","dateCreated":"2020-01-22T17:37:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3029","dateFinished":"2020-01-22T22:19:59+0000","dateStarted":"2020-01-22T22:19:59+0000"},{"user":"anonymous","dateUpdated":"2020-01-22T22:20:21+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579714639569_1723581077","id":"20200118-172655_2069013167","dateCreated":"2020-01-22T17:37:19+0000","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:3030","text":"writeIntoMongodb(gkgDF_q3_cleaned, \"query3\")","dateFinished":"2020-01-22T22:26:06+0000","dateStarted":"2020-01-22T22:20:21+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job 63 cancelled part of cancelled job group zeppelin-anonymous-2EZGWZ1RA-20200118-172655_2069013167\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1976)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:946)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:946)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:946)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:946)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2231)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:935)\n  at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:933)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n  at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:933)\n  at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:117)\n  at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:159)\n  at com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:73)\n  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:156)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n  at com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:192)\n  at $$$82b5b23cea489b2712a1db46c77e458$$$$w$writeIntoMongodb(<console>:218)\n  ... 104 elided\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1579731621441_681062800","id":"20200122-222021_1036461751","dateCreated":"2020-01-22T22:20:21+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4863"}],"name":"gdeltExploration-jperes","id":"2EZGWZ1RA","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"python:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}